{"pages":[],"posts":[{"title":"ConcurrentHashMap 1.7与1.8区别","text":"1、整体结构1.7：Segment + HashEntry + Unsafe（数组+链表） 1.8: 移除Segment，使锁的粒度更小，Synchronized + CAS + Node + Unsafe （数组+链表+红黑树） 2、put（）1.7：先定位Segment，再定位桶，put全程加锁，没有获取锁的线程提前找桶的位置，并最多自旋64次获取锁，超过则挂起。 1.8：由于移除了Segment，类似HashMap，可以直接定位到桶，拿到first节点后进行判断， 为空则CAS插入； 为-1则说明在扩容，则跟着一起扩容； else则加锁put（类似1.7） 3、get（）基本类似，由于value声明为volatile，保证了修改的可见性，因此不需要加锁。 4、resize（）1.7：跟HashMap步骤一样，只不过是搬到单线程中执行，避免了HashMap在1.7中扩容时死循环的问题，保证线程安全。 1.8：支持并发扩容，HashMap扩容在1.8中由头插改为尾插（为了避免死循环问题），ConcurrentHashmap也是，迁移也是从尾部开始，扩容前在桶的头部放置一个hash值为-1的节点，这样别的线程访问时就能判断是否该桶已经被其他线程处理过了。 5、size（）1.7：很经典的思路：计算两次，如果不变则返回计算结果，若不一致，则锁住所有的Segment求和。 1.8：用baseCount来存储当前的节点个数，这就设计到baseCount并发环境下修改的问题。","link":"/2020/04/20/ConcurrentHashMap-1-7%E4%B8%8E1-8%E5%8C%BA%E5%88%AB/"},{"title":"gnuradio_ubuntu16","text":"GNU Radio 1.更新软件包列表，更新系统软件 sudo apt-get update sudo apt-get upgrade 2. 安装依赖包(仅适用于UBUNTU 16.04，其他版本需要的依赖包会有所不同)sudo apt-get install libfontconfig1-dev libxrender-dev libpulse-dev swig g++ automake autoconf libtool python-dev libfftw3-dev libcppunit-dev libboost-all-dev libusb-dev libusb-1.0-0-dev fort77 libsdl1.2-dev git-core libqt4-dev python-numpy ccache python-opengl libgsl-dev python-cheetah python-mako python-lxml doxygen qt4-default qt4-dev-tools libusb-1.0-0-dev libqwt5-qt4-dev libqwtplot3d-qt4-dev pyqt4-dev-tools python-qwt5-qt4 cmake git-core wget libxi-dev python-docutils gtk2-engines-pixbuf r-base-dev python-tk liborc-0.4-0 liborc-0.4-dev libasound2-dev python-gtk2 libzmq-dev python-requests python-sphinx python-zmq libncurses5 libncurses5-dev python-wxgtk3.0 python-scipy python-matplotlib 3. 下载UHD 源文件并编译安装（选择你认为合适的文件目录下）==PS：一定要先安装uhd不然会没有usrp模块== git clone git://github.com/EttusResearch/uhd.git（若提示git 软件未安装，则按照提示输入sudo apt-get install git 安装） 下载的源文件有不同release 版本，通过如下操作找到最新的release 版本或者你需要的版本：cd uhd然后在终端输入git checkout release 连续按tab 键，将会打印出可选的release 版本，然后输入git checkout release_003_011_000_000（切换到合适版本，这里选择3.11.0） cd host mkdir build cd build cmake ../ make 使用make -j4并行编译 make test sudo make install sudo ldconfig 4. 下载UHD 镜像文件and烧写镜像文件。==要是usrp已经烧写过镜像可省略==sudo uhd_images_downloader 如果你安照步骤操作，编译，安装不会报错。此时UHD驱动已经安装完毕，可通过输入uhd_find_devices 可看到USRP设备的信息（确保电脑与设备处于同一网络下。注意usrp n210必须使用千兆网线与pc段通信，确保pc机有千兆网卡，且网线为千兆网线。当时在这里走了弯路。具体设置查看http://www.ettus.com.cn/peixun/28/）uhd_image_loader –args=”type=usrp2,addr=192.168.10.2（默认的ip地址）,reset” 5. 下载GNU Radio 源文件并编译安装（步骤类似,因为都是下载源文件,安装啊，选择合适文件夹） git clone –recursive git://github.com/gnuradio/gnuradio.git cd gnuradio/ 切换到release 版本，可以通过以下方式查找最新的release 版本：git checkout v3 连续按tab 键，将会打印出可选的release 版本，找到版本号最高的版本，比如v3.7.9.1。 切换到该最高版本：git checkout v3.7.9.1 ==PS：一定要切换到最新版本== mkdir build cd build cmake ../ make(有些会编译失败，不影响使用）==make -j4 使用4个线程编译 速度会提升大约20分钟即可编译完成== make test sudo make install sudo ldconfig 如果一切顺利，到此就已经ok了。 终端下输入 gnuradio-companion就能打开软件了，此时你自己写一个小程序或者打开一个example.grc。网上关于”hello，world“似的例子有很多，百度即可。","link":"/2019/04/09/gnuradio-install/"},{"title":"python_with_matlab","text":"前言 近期chirp—ook背向通信工作遇到了瓶颈。tag的硬件上FPGA反射OOK不够精确其他原因，通信工作暂停。现转为无线感知，具体为基于chirp信号的backscatter的动作识别。 考虑动作识别，那么ML必然逃不过。根据先前的RFID定位的经验，任何复杂情景下的感知，ML都较于传统模型来的快。老板一直说不要吃快餐，可是别说是快餐，在不学习ML可能连毕业都是问题。 由于处理信号数据matlab有天然的优势，而deeplearning又逃不过python。于是python与matlab的交换使用不失为一种解决方案！ 1. 安装 Python3.6 + anaconda3 + Matlab2018a-转自Python 的 MATLAB 引擎 API- 要在 Python® 会话内启动 MATLAB® 引擎，必须先安装 Python 包形式的引擎 API。MATLAB 提供了标准的 Python setup.py 文件，用于通过 distutils 模块编译和安装引擎。您可以使用相同的 setup.py 命令在 Windows®、Mac 或 Linux® 系统上编译和安装引擎。 在安装之前，确认您的 Python 和 MATLAB 配置。 检查您的系统是否具有受支持的 Python 版本和 MATLAB R2014b 或更新版本。要检查您的系统上是否已安装 Python，请在操作系统提示符下运行 Python。 将包含 Python 解释器的文件夹添加到您的路径（如果尚未在该路径中）。 找到 MATLAB 文件夹的路径。启动 MATLAB，并在命令行窗口中键入 matlabroot。复制 matlabroot 所返回的路径。 要安装引擎 API，请选择以下选项之一。 在 Windows 操作系统提示符下 - 12cd &quot;matlabroot\\extern\\engines\\python&quot;python setup.py install 您可能需要管理员权限才能执行这些命令。 在 macOS 或 Linux 操作系统提示符下 - 12cd &quot;matlabroot/extern/engines/python&quot;python setup.py install 2. 使用 如果你要自定义一个函数(function.m)，需将该文件放在于python工程下。 下例为使用matlab提供的python API，读一个二进制复数文件(信号的IQ信息)。 123456789101112import matplotlib.pyplot as pltimport matlab.enginedef data_input(filename): eng = matlab.engine.start_matlab() data = eng.read_complex_binary(filename,1e7) plt.plot(eng.abs(data[1:10**5])) plt.show()if __name__ == '__main__': data_input('./data/0423/gesture1.dat') 123456789101112131415161718192021222324252627function v = read_complex_binary (filename, count) %% usage: read_complex_binary (filename, [count]) %% %% open filename and return the contents as a column vector, %% treating them as 32 bit complex numbers %% m = nargchk (1,2,nargin); if (m) usage (m); end if (nargin &lt; 2) count = Inf; end f = fopen (filename, 'rb'); if (f &lt; 0) v = 0; else t = fread (f, [2, count], 'float'); fclose (f); v = t(1,:) + t(2,:)*i; [r, c] = size (v); v = reshape (v, c, r); end 信号的时域图 3. 注意！ 网上有很多教程为mlab的安装教程，但mlab十分不适用于python3+。官方的文档给的版本为python2.7。尝试了很久，安装都不成功。个人不介意使用mlab。","link":"/2019/04/23/python-with-matlab/"},{"title":"卡尔曼与粒子滤波","text":"现有⼀辆在路上做直线运动的小车。假设小车k时刻的速度为$u_{k}$，设k-1时刻小车的位置为$x_{k-1}$，则k时刻小⻋的位置$x_{k}$ 为：$$x_{k}=x_{k-1}+\\nabla t * u_{k}+w_{k}$$ 其中$w_{k}$ 为过程噪声，$w_{k} \\sim N(0, Q)$， GPS在k时刻对小车行驶距离的观测值为$y_{k}$，GPS存在测量噪声$v_{k} \\sim N(0, R)$ $$y_{k}=x_{k}+v_{k}$$ 现⽤卡尔曼滤波算法对小车的行驶距离进⾏估计： 写出k时刻小车行驶距离估计值的更新迭代方程 ： $$ \\hat x_{k}^{-}=F_{k} {x}_{k-1} \\tag{1} $$ $$ \\Sigma_{k}^{-}=F \\Sigma_{k-1} F^{T}+Q \\tag{2} $$ $$ K_{k}=\\Sigma_{k}^{-} H^{T}\\left(H \\Sigma_{k}^{-} H^{T}+R\\right)^{-1} \\tag{3} $$ $$ \\hat x_{k} = \\hat x_{k}^{-}+K_{t} \\left (y_{k}-H \\hat x_{k}^{-} \\right) \\tag{4} $$ $$ \\Sigma_{k}=\\left(I-K_{k} H\\right) \\Sigma_{k}^{-} \\tag{5} $$ 其中（1）（2）为预测方程，（3）（4）（5）为更新迭代方程。$\\hat x_{k}^{-}=\\begin{bmatrix} p_{k} \\\\ v_{k} \\\\\\end {bmatrix} =\\begin {bmatrix} 1 &amp; \\Delta t \\\\ 0 &amp; 1\\\\\\end {bmatrix}\\begin {bmatrix}p_{k-1} \\\\v_{k-1} \\\\\\end {bmatrix}$，$\\quad F_{k}=\\begin {bmatrix} 1 &amp; \\Delta t \\\\ 0 &amp; 1 \\\\\\end {bmatrix}$，其中 $F_{k}$ 为状态转移矩阵，$\\boldsymbol{H}=\\left[ \\begin{array}{ll}{1} &amp; {0}\\end{array}\\right]$ 。 $\\Sigma_{k}$ 为协方差矩阵，表示每一时刻的不确定性。 $K$ 为卡尔曼系数，为一个矩阵，是对残差的加权矩阵，也称滤波增益矩阵。 $Q$ ,$R$ 分别为预测噪声与测量噪声。 如果测量噪⾳很小，考虑极端情况，测量噪声的⽅差$R$趋向于0，对小车行驶距离的估计受测量值影响较大还是估计值影响较z？写出推导过程。 $lim_{R \\rightarrow 0} K_{k}=H^{-1}$，观测噪声协方差$R$ 越小，残余的增益越大 K 越大。$y_{k}$ 的权重变大，即受测量值影响大。 相反，当$Q$ 趋向于0，有：$lim_{Q \\rightarrow 0} K_{k}=0$ ，$\\hat x_{k}^{-}$ 的权重变大，即受估计值影响大。 PS: 公式（3）的推导见博客卡尔曼滤波 – 从推导到应用(一) 粒子滤波习题 有⼀可以在⼆维平⾯上移动的小车（图中灰⾊圆点）。每隔固定的⼀段时间⽤粒子滤波算法定位小车的位置。有四个粒⼦x1，x2，x3，x4，x1[(1,5),0.25]，x2[(3,5),0.25]，x3[(3,7),0.25]，x4[(5,5),0.25]（x[(横坐标x，纵坐标y)，权重w]，图中红⾊星星）。 假设我们现在知道了车的运动向量为(5,4)（图中蓝色箭头）。求小车位置的预测值。 (1,5) * 0.25 + (3,5) * 0.25 + (3,7) * 0.25 + (5,5) * 0.25 + (5,4) = (8,9.5) 此时小车上GPS对小车位置的定位为(8,10)，GPS的测量误差服从⼆维标准正态分布。请按照GPS对定位的测量值对四个粒⼦重新分配权重。 12345678910111213141516171819202122232425262728293031import numpy as npimport scipy.statsfrom numpy.random import uniform, randn, randomdef update(particles, weights, z, R, landmarks): weights.fill(1.) for i, landmark in enumerate(landmarks): distance = np.linalg.norm(particles[:, 0:2] - landmark, axis=1) weights *= scipy.stats.norm(distance, R).pdf(z[i]) weights += 1.e-300 # avoid round-off to zero weights /= sum(weights) # normalizeif __name__ == '__main__': vector = np.array([[5,4]]) #移动向量 particles = np.array([[1,5],[3,5],[3,7],[5,5] ]) particles += vector #粒子位置 weights = np.array([0.25,0.25,0.25,0.25]) #初始权重 pos =np.array( [[8,9.5]]) #预测位置 landmark = np.array([[8,10]]) #GPS定位（测量位置） NL = len(landmark) sensor_std_err = 1 #测量误差（二维标准正态分布） z = np.linalg.norm(pos-landmark,axis=1) #z = dist(预测位置，测量位置) update(particles,weights,z,sensor_std_err,landmark) print(weights) 重新分配的权重为：$[0.10034672,0.39965328,0.39965328,0.10034672]$ 粒子更新为：$[[ 6 , 9] [ 8 , 9] [ 8 ,11] [10 , 9]]$ 在前两问的基础上，写出重采样算法估计小车的位置的伪代码. 采用多项式重采样，基本步骤如下： $$ F(x)=P(X \\leq x)=\\sum_{x^{i}&lt;x} p\\left(x_{i}\\right) $$ 将 $[0,1]$ 区间分为 $N$ 个子区间 $\\left(0, p\\left(x^{1}\\right)\\right]$ ，$\\left(p\\left(x^{1}\\right), p\\left(x^{1}\\right)+\\right.$$p\\left(x^{2}\\right) ]$，···，$\\left(\\sum_{m=1}^{N-1} p\\left(x^{m}\\right), \\sum_{m=1}^{N} p\\left(x^{m}\\right)\\right]$。设$U$是 $[0,1]$ 区间上的均匀分布时间变量，根据$U$的值落到何区间，相应的区间对应的随机变量就是所需输出量，假设落在第$j$区间，则输出为$x^{j}$，粒子的子代数 $n^{i}$ 表示 $U$ 的值落在该区间的次数。 伪代码如下： 产生[0,1]上均匀分布随机数 $\\left\\{u_{i}\\right\\}_{j=1, n}$ ; 产生粒子权重累积函数$wc$，满足$w c(i)=\\sum_{m=1}^{i} w_{k}^{m}$ ; k=1; for i=1:N while (wc(k)&lt;u(i)) k = k+1; end index(i) = k; //表示第k个粒子经重采样后被复制在第i个位置 end 运行代码如下： 12345678910def simple_resample(particles, weights): N = len(particles) cumulative_sum = np.cumsum(weights) cumulative_sum[-1] = 1. # avoid round-off error indexes = np.searchsorted(cumulative_sum, random(N)) # resample according to indexes particles[:] = particles[indexes] weights[:] = weights[indexes] weights /= np.sum(weights) # normalize PS：部分代码转自机器人粒子滤波定位（蒙特卡罗定位）","link":"/2019/05/02/%E5%8D%A1%E5%B0%94%E6%9B%BC%E4%B8%8E%E7%B2%92%E5%AD%90%E6%BB%A4%E6%B3%A2/"},{"title":"java线程池ThreadPoolExecutor","text":"线程池优点重用性：，减少对象创建、消亡的开销，性能佳。并发控制：提高系统资源的使用率，同时避免过多资源竞争，避免堵塞。 ThreadPoolExecutor1234567public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) {} corePoolSize线程池的核心线程数。在没有设置allowCoreThreadTimeOut为true的情况下，核心线程会在线程池中一直存活，即使处于闲置状态。 maximumPoolSize线程池允许创建的最大线程数。 keepAliveTime非核心线程闲置时的超时时长。超过该时长，非核心线程就会被回收。若allowCoreThreadTimeOut属性为true时，该时长同样会作用于核心线程。 unit：keepAliveTime的时间单位。 workQueue：线程池中的任务队列，通过线程池的execute()方法提交的Runnable对象会存储在该队列中。 可选子类： threadFactory： 线程工厂， RejectedExecutionHandler： 当队列和线程池满了，拒绝策略4种 1、抛异常(AbortPolicy) （默认） 2、丢弃队列中最老的任务(DiscardOldestPolicy)。 3、直接丢弃（DiscardPolicy） 4、将任务分给调用线程来执行(CallerRunsPolicy)。 处理流程 Executors提供的线程池 FixedThreadPool线程数量固定的线程池，无限的任务队列，只有核心线程。最多只有nThreads个任务在并行处理，之后都在排队等待。 12345public static ExecutorService newFixedThreadPool(int nThreads) { return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;());} CachedThreadPool适合执行大量耗时较少的任务。没有核心线程，即没有任务时，它几乎不占用任何系统资源。SynchronousQueue不缓存任何一个任务，当即执行。 12345public static ExecutorService newCachedThreadPool() { return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;());} ScheduledThreadPool 创建一个定长线程池，支持定时及周期性任务执行。 12345678public static ScheduledExecutorService newScheduledThreadPool(int corePoolSize) { return new ScheduledThreadPoolExecutor(corePoolSize); } public ScheduledThreadPoolExecutor(int corePoolSize) { super(corePoolSize, Integer.MAX_VALUE, 10L, MILLISECONDS, new DelayedWorkQueue()); } SingleThreadExecutor 创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行。 123456public static ExecutorService newSingleThreadExecutor() { return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()));}","link":"/2020/04/22/java%E7%BA%BF%E7%A8%8B%E6%B1%A0ThreadPoolExecutor/"},{"title":"python常见api","text":"刷题常用api对一些常见的数据结构以及算法的api总结 一： list12345678910111213141516171819202122232425262728--- cmp(list1, list2)比较两个列表的元素--- len(list)列表元素个数--- max(list)返回列表元素最大值--- min(list)返回列表元素最小值--- list(seq)将元组转换为列表--- list.append(obj)在列表末尾添加新的对象--- list.count(obj)统计某个元素在列表中出现的次数--- list.extend(seq)在列表末尾一次性追加另一个序列中的多个值（用新列表扩展原来的列表）--- list.index(obj)从列表中找出某个值第一个匹配项的索引位置--- list.insert(index, obj)将对象插入列表--- list.pop([index=-1])移除列表中的一个元素（默认最后一个元素），并且返回该元素的值--- list.remove(obj)移除列表中某个值的第一个匹配项--- list.reverse()反向列表中元素--- list.sort(cmp=None, key=None, reverse=False)对原列表进行排序 二： dict1234567891011121314151617181920212223242526272829303132--- cmp(dict1, dict2)比较两个字典元素。--- len(dict)计算字典元素个数，即键的总数。--- str(dict)输出字典可打印的字符串表示。--- type(variable)返回输入的变量类型，如果变量是字典就返回字典类型。--- dict.clear()删除字典内所有元素--- dict.copy()返回一个字典的浅复制--- dict.fromkeys()创建一个新字典，以序列seq中元素做字典的键，val为字典所有键对应的初始值--- dict.get(key, default=None)返回指定键的值，如果值不在字典中返回default值--- key in dict如果键在字典dict里返回true，否则返回false--- dict.items()以列表返回可遍历的(键, 值) 元组数组--- dict.keys()返回一个迭代器，可以使用 list() 来转换为列表--- dict.setdefault(key, default=None)和get()类似, 但如果键不存在于字典中，将会添加键并将值设为default--- dict.update(dict2)把字典dict2的键/值对更新到dict里--- dict.values()返回一个迭代器，可以使用 list() 来转换为列表--- pop(key[,default])删除字典给定键 key 所对应的值，返回值为被删除的值。key值必须给出。 否则，返回default值。--- popitem()随机返回并删除字典中的一对键和值(一般删除末尾对)。 三： set 添加元素x 可以有多个，用逗号分开。s.add( x )还有一个方法，也可以添加元素，且参数可以是列表，元组，字典等，语法格式如下：s.update( x ) 移除元素语法格式如下：s.remove( x )此外还有一个方法也是移除集合中的元素，且如果元素不存在，不会发生错误。格式如下所示：s.discard( x )我们也可以设置随机删除集合中的一个元素，语法格式如下：s.pop() 计算集合元素个数语法格式如下：len(s) 清空集合语法格式如下：s.clear() 判断元素是否在集合中存在语法格式如下：x in s 四：常用方法： 排序用lst.sort() 而不是nlst = sorted(lst)区别在于lst.sort()是 in-place sort，改变lst, sorted会创建新list，成本比较高。 用xrangexrangge 和 range的区别是在于 range会产生list存在memory中，xrange更像是生成器，generate on demand所以有的时候xrange会更快 三目运算符python 的三目运算符是这么写的 x if y else z考虑这种 matrix = [ [1,2,3] , [4,5,6] ]row = len(matrix)col = len(matrix[0]) if row else 0这样写通用的原因是， 当matrix = [], row = 0, col =0 list 填 0lst = [0 for i in range(3)] # lst = [0,0,0]lst = [[0 for i in range(3)] for j in range(2)] # lst = [[0, 0, 0], [0, 0, 0]]","link":"/2020/04/26/python%E5%B8%B8%E8%A7%81api/"},{"title":"机器学习:决策树","text":"本文尽量采用最简洁最直白的描述，对于有些解释有出入的恳请指出。 前言：经过两次算法的面试，发现工业界对于机器学习主要是在分布式和效率上，对于精度的取舍可能不是很看重。因此，集成学习和深度学习是要掌握透彻的。（XGboost、GBDT和RF几乎必问） 什么是决策树？决策树是一种采用属性(特征)划分来解决分类问题的算法，通常有3个步骤： 特征选择：对于多维特征，依次计算不同维度上特征的信息增益。 决策树生成：选取最大信息增益的特征作为根节点，依次生成子节点。 剪枝：对抗过拟合，去除分支。（预剪枝，后剪枝） 3种决策树(划分)算法 ID3：采用信息增益，最早的决策树算法 C4.5：采用信息增益比 CART(Classification and regression Tree)：采用基尼系数 优缺点 优点 缺点 运行速度快，能够应对大型数据源 易过拟合 适合有缺失属性的样本 忽略特征之间的关联 可以同时处理标称型与数值型数据 类别样本不均衡时，ID3偏好数目多的属性，CART偏好数目少的属性 可视化 连续和缺失值 连续值：类型属性离散化，如二分法 缺失值： 如何属性缺失情况下划分？（给定样本权重，权重等价于属性缺失占的比例） 给定划分后属性后，样本的属性缺失如何划分？（让同一个样本以不同概率分到不同子节点） 机器学习—RF、GBDT、XGBoost前言：RF、GBDT、XGBoost都是集成学习，即组合多个基学习器预测结果来得到最终的结果，提高了模型的泛化性和鲁棒性。分为两类： 基学习器之间强依赖，必须串行生成：Boosting 基学习器之前弱依赖，可并行生成：Bagging、RF Bagging与随机森林原理：相互交替的采用子集训练不同的学习器。主要关注降低方差，在不剪枝决策树、神经网络等易受样本扰动的学习器上效用更为明显。 Bagging： 放回抽样 多数表决（分类）或简单平均（回归） RF： 随机选择样本（放回抽样） 随机选择特征 构建决策树 随机森林投票（平均）。 Boosting原理：不断迭代，调整分布。主要关注降低偏差，提高泛化。 初学习器 调整分布，使得错误的样本有更高的关注（加权） 基于调制后的分布，循环训练下一个基训练器，直到T次循环结束。（生成新学习器） 结合策略平均法：简单平均、加权平均 投票法：绝对多数投票、相对多数投票、加权投票 学习法：通过元学习器结合 GBDTGBDT与传统的Boosting区别较大，它的每一次计算都是为了减少上一次的残差，在残差减小的梯度方向上建立模型。 所以说，在Gradient Boost中，每个新的模型的建立是为了使得之前的模型的残差往梯度下降的方法，与传统的Boosting中关注正确错误的样本加权有着很大的区别。GBDT的会累加所有树的结果，而这种累加是无法通过分类完成的，因此GBDT的树都是CART回归树，而不是分类树。 XGBoostXGBoost利用并行的CPU解决迭代次数过多问题 与GBDT区别 基学习器：GBDT以CART树作为基学习器，XGBoost还支持线性分类器，这个时候XGBoost相当于L1和L2正则化的逻辑斯蒂回归（分类）或者线性回归（回归）； 代价函数求导：传统的GBDT在优化的时候只用到一阶导数信息，XGBoost则对代价函数进行了二阶泰勒展开，得到一阶和二阶导数； 正则化：XGBoost在代价函数中加入了正则项，用于控制模型的复杂度。从权衡方差偏差来看，它降低了模型的方差，使学习出来的模型更加简单 缺失值的处理：XGBoost还可以自动学习出它的分裂方向 并行：多线程进行各个特征的增益计算（非Tree粒度的并行）","link":"/2020/04/20/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%86%B3%E7%AD%96%E6%A0%91/"},{"title":"基于频繁项集的车伴随问题","text":"题目：主要道路十字路口的车牌抓拍相机会把所有车辆的车牌号采集到后台，需要计算所有车辆的轨迹数据中，哪些车是伴随出行，比如2辆车一起自驾出游，采集的轨迹为T（P1、P2、P3…..）轨迹点数据格式为P（路口编号，时间戳，车牌号）。 1、目的：从所有车辆数据中挖掘出车与车之间的伴随关系. 2、已有的条件：每辆车的轨迹数据T1（p1、p2、p3。。。），P（位置编号、时机戳、车牌号） 3、需要计算一段时间内（2周或者1个月）两辆车的伴随的强度分数（1-100分），请基于这种场景给出算法思路，以及算法可能遇到的挑战？ 问题重述点伴随问题是指通过了解哪些车辆频繁地在多个监测点共同出现来分析它们之间的相互关系，本质上就是寻找不同车辆之间的关联性或相关性，因此可以使用关联分析方法来解决。问题因此可以通过对点伴随组进行关联分析，找出满足这一概率的频繁子集车辆，即可求解出伴随车辆组。 问题挑战问题转换为点伴随组关联分析，目前传统的频繁项集挖掘主要包括两大类算法，基于Apriori的挖掘算法和基于模式增长(FP-growth)类的算法。 Apriori算法是需要迭代并产生候选项集，与并行计算相比，它们对大数据的处理效果更好，但需要更多时间才能完成，在实时性上有较大缺陷。 FP-growth算法不产生候选项集，且只需要两次扫描数据集，第一次扫描数据集找到频繁项的列表，第二次扫描每个事务不断构建FP-tree然后对毎个频繁项对应的条件模式树进行挖掘，最终得到频繁项集。但是，FP-tree是基于内存构建的，随着树的深度增加，所需内存和消耗CPU资源也急剧増加。 可以总结挑战为两种：内存和响应时间。 算法现阶段研究有提出一种基于Spark Streaming 分布式流数据处理系统实现的并行算法。下文将简述该方案。 数据预处理 ​ 将流数据ANPR进行数据清洗，提取车辆轨迹T1(p1,p2,p3…)，P(v,s,t)，v:车牌号，s:位置编号，t:时间戳。 伴随车辆组 ​ 伪代码如下： 1234567891011121314151617181920def TravelingCom(): #伴随车辆组 for RDDStream_i in RDDStream: #RDDStream为滑动窗口得到的一段时间内的P集合 (s,&lt;v,t&gt;) = mapToPair(RDDStream_i) #map映射从RDD集合中得到（s,v,t) (s,List(v)) = reduceByKey(s,&lt;v,t&gt;) #提取该窗口内该位置和对应车牌号 G_max = getTravelingCom(s,List(v),com) #提取最大伴随测量组,com为监测点阈值(支持度) (List(s),List(v)) = mapToPair(G_max) #map映射得到伴随车辆组 Q.add((List(s),List(v)) return Q #返回结果 def getTravelingCom(s,List(v),com): #最大伴随关系集合 itemsize = len(k_itemset) #k_itemset为k项频繁项集，初始k=1，即所有数 if itemsize &lt; 2: #据都为频繁1项集 Q_max.add(k_itemset) for i in range(0,itemsize): for j in range(i+1,itemsize): List(v)_ij = List(v)_i &amp; List(v)_j #求两两频繁项集的交集 if len(List(v)_ij) &gt; com: (k+1)_itemset.add((s_i,s_j),List(v)_ij) Q_max.add((k+1)_itemset) return Q_max 分数计算 ​ 伪代码如下： 12345def Score(v_i,v_j): num = Q.find(v_i,v_j) #求支持度 score = num/len(Q) score = Normalization(score,0,100) #标准化得到（0,100)分数 return score 算法分析​ 概述：根据车牌识别流数据稀疏的特征，避免使用递归，而是使用循环求得频繁项集的交集作为候选集，并给出支持度阈值避免内存溢出。 ​ 优点：内存占用少，时间响应快。 ​ 缺点：需调整支持度阈值、时间窗口长度来达到最优。 ​ 给出建议：可以使用ML方法学习得到最佳参数(支持度阈值，窗口长度等等)。 参考文献基于车牌流数据的伴随车发现方法研究 A Novel Integrated Approach for Companion Vehicle Discovery Based on Frequent Itemset Mining on Spark A Service-Friendly Approach to Discover Traveling Companions Based on ANPR Data Stream","link":"/2020/04/24/%E5%9F%BA%E4%BA%8E%E9%A2%91%E7%B9%81%E9%A1%B9%E9%9B%86%E7%9A%84%E8%BD%A6%E4%BC%B4%E9%9A%8F%E9%97%AE%E9%A2%98/"},{"title":"java常见api","text":"其实用IDE就自带API，也就当总结一下吧。 &lt;more&gt; ArrayList ：常用的动态数组 123456789101112131415161718192021222324//声明ArrayList&lt;&gt; list = new ArrayList&lt;&gt;();//返回ArrayList的实际大小 public int size(); //判断ArrayList是否为空 public boolean isEmpty(); //判断ArrayList是否包含元素o public boolean contains(Object o); //正向查找，返回元素的索引值 public int indexOf(Object o); //反向查找，返回元素的索引值 public int lastIndexOf(Object o); //获取index位置的元素 public E get(int index); //将e添加到ArrayList末尾 public boolean add(E e); //删除ArrayList指定位置的元素 public E remove(int index); //删除ArrayList中指定的元素 public boolean remove(Object o); //清空ArrayList，将全部元素置为null public void clear(); //将集合C中的所有元素添加到ArrayList中 public boolean addAll(Collection&lt;? extends E&gt; c); LinkedList ：常用的双向动态链表（可以充当stack和queue） 123456789101112131415161718192021222324252627282930313233343536373839404142//声明LinkedList&lt;&gt; list = new LinkedList&lt;&gt;();//增 //以index为插入下标，插入集合c中所有元素 public boolean addAll(int index, Collection&lt;? extends E&gt; c); //在指定下标，index处，插入一个节点 public void add(int index, E element); //尾追加 public boolean offer(E element); public boolean offerLast(E element); //头添加 public boolean offerFirst(E element);//删 //删除指定节点 public boolean remove(Object o); //删除指定index public E remove(int index); //删除表头 public E removeFirst(); public E poll(); public E pollFirst(); //删除表尾 public E removeLast(); public E pollLast();//改 //修改index的值（先遍历再改） public E set(int index, E element);//查 //取表头 public E getFirst(); public E peek(); public E peekFirst(); //取表尾 public E getLast(); public E peekLast(); //根据对象查index(顺序遍历) public int indexOf(E element); //根据对象查index(逆序遍历) public int lastIndexOf(E element); //转为数组public E[] toArray();","link":"/2020/05/09/java%E5%B8%B8%E8%A7%81api/"},{"title":"2021重新出发","text":"2021年是个不同寻常的一年，从2020年的疫情爆发以来，整个社会都发生着天翻地覆的变化。也许再过几十年回头看，这也许是中国转折的一个节点。当然，21年也是我党成立100周年，值得纪念的一年。国家是走向强盛还是 衰退不得而知，但是机遇与挑战却昭然若揭。赶上时代的潮流，仿佛都是大家的想法，但踏踏实实的积累才是不败之法。 说了那么多，聊聊个人。2021年6月份，我毕业了。这意味着校园生活画上句号了。欸，有点不舍… 虽然畅享过生活会变得丰富多彩的，但现实却没有乌托邦。工作后的生活如同一滩死水，唯有解决问题或完成目标能带来些许欣慰。不过，转而代替的却是长而平淡的循环和循环。我经常挂在嘴上的一句话：”if you want to do, just do it”。遵循内心，我从底层开发转向上层集成的小组。当然，选择本身就带有一定不确定性。这次选择只能在这短暂的周期内，寄于它期待吧。 很久没有静下心来写些文字了，直到写这里，我的内心却仍然保有浮躁。当然，时间会给我答案的。","link":"/2021/08/09/2021%E9%87%8D%E6%96%B0%E5%87%BA%E5%8F%91/"}],"tags":[{"name":"Java","slug":"Java","link":"/tags/Java/"},{"name":"comm","slug":"comm","link":"/tags/comm/"},{"name":"python","slug":"python","link":"/tags/python/"},{"name":"java","slug":"java","link":"/tags/java/"},{"name":"算法","slug":"算法","link":"/tags/%E7%AE%97%E6%B3%95/"},{"name":"个人感受","slug":"个人感受","link":"/tags/%E4%B8%AA%E4%BA%BA%E6%84%9F%E5%8F%97/"}],"categories":[]}